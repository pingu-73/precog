{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e27a7e-8958-4935-828f-80389b642e80",
   "metadata": {},
   "source": [
    "### Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e1177e-0039-4014-801c-4f10bbd3d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cdaee7-ebf8-41fe-9d7f-539b85afcf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>This was a series of nested angular standards ...</td>\n",
       "      <td>This was a series of nested polar scales , so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>His father emigrated to Missouri in 1868 but r...</td>\n",
       "      <td>His father emigrated to America in 1868 , but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>In January 2011 , the Deputy Secretary General...</td>\n",
       "      <td>In January 2011 , FIBA Asia deputy secretary g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Steiner argued that , in the right circumstanc...</td>\n",
       "      <td>Steiner held that the spiritual world can be r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Luciano Williames Dias ( born July 25 , 1970 )...</td>\n",
       "      <td>Luciano Williames Dias ( born 25 July 1970 ) i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentence1  \\\n",
       "0   1  This was a series of nested angular standards ...   \n",
       "1   2  His father emigrated to Missouri in 1868 but r...   \n",
       "2   3  In January 2011 , the Deputy Secretary General...   \n",
       "3   4  Steiner argued that , in the right circumstanc...   \n",
       "4   5  Luciano Williames Dias ( born July 25 , 1970 )...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  This was a series of nested polar scales , so ...      0  \n",
       "1  His father emigrated to America in 1868 , but ...      0  \n",
       "2  In January 2011 , FIBA Asia deputy secretary g...      1  \n",
       "3  Steiner held that the spiritual world can be r...      0  \n",
       "4  Luciano Williames Dias ( born 25 July 1970 ) i...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from datasets import load_dataset\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "df = load_dataset('paws', 'labeled_final')\n",
    "\n",
    "train = df['train'].to_pandas()\n",
    "validation = df['validation'].to_pandas()\n",
    "test = df['test'].to_pandas()\n",
    "# train.head()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2207bb-eb45-415c-9709-aba9c4f809fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "# train_sentence1 = train['sentence1'][0:1500].tolist()\n",
    "# train_sentence2 = train['sentence2'][0:1500].tolist()\n",
    "# train_id = train['id'].tolist()\n",
    "\n",
    "test_sentence1 = train['sentence1'][0:1500].tolist()\n",
    "test_sentence2 = train['sentence2'][0:1500].tolist()\n",
    "test_id = test['id'].tolist()\n",
    "\n",
    "\n",
    "# train_sentence1\n",
    "vectorizer = Vectorizer()\n",
    "# vectorizer.run(train_sentence1)\n",
    "# vectors_TS1_bert = vectorizer.vectors\n",
    "\n",
    "# vectorizer.run(train_sentence2)\n",
    "# vectors_TS2_bert = vectorizer.vectors\n",
    "\n",
    "vectorizer.run(test_sentence1)\n",
    "vectors_TST1_bert = vectorizer.vectors\n",
    "\n",
    "vectorizer.run(test_sentence2)\n",
    "vectors_TST2_bert = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f21dda-5bb4-4065-9e6e-db07a65cf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5dc17b6-0fe0-4d90-8788-e146515ed6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4418736462824639"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = sum(train['label'] == 1)\n",
    "total_pairs = (max(train['id']))\n",
    "threshold = count/total_pairs\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f506e9b6-7293-4d30-867e-5e3776e562d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrasing = []\n",
    "# for i in vectors_TS1_bert, vectors_TS2_bert,train['label'][0:1500]:\n",
    "#     similarity_score = cosine(vectors_TS1_bert[i], vectors_TS2_bert[i]) \n",
    "#     if similarity_score >= threshold:\n",
    "#         paraphrasing.append(1)\n",
    "#     else:\n",
    "#         paraphrasing.append(0)\n",
    "\n",
    "for i in range(len(vectors_TST1_bert)):\n",
    "    similarity_score = cosine_similarity([vectors_TST1_bert[i]], [vectors_TST2_bert[i]])[0][0]\n",
    "    # print(similarity_score)\n",
    "    if similarity_score >= threshold:\n",
    "        paraphrasing.append(1)\n",
    "    else:\n",
    "        paraphrasing.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b407d6-af73-43cd-b47f-fb6efc3c6ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors_TST1_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93a1acb-9fbc-4358-9e4e-7e36adfd3502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrasing.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded33cc-3364-4e89-a3f9-70c8fa310131",
   "metadata": {},
   "source": [
    "Since all the words are paraphrases so they have similarity socre of 1 so to differentiate that we can use **parts of speech taging** and **check if all the parts of speech match or not**.\n",
    "\n",
    "Since all the sentences are paraphrases $\\implies$ they are supposed to be same\n",
    "for example:\n",
    "sentence-1: This was a series of nested **angular standards** , so that measurements in azimuth and elevation could be done directly in polar coordinates relative to the ecliptic .\n",
    "sentence-2: This was a series of nested **polar scales** , so that measurements in azimuth and elevation could be performed directly in angular coordinates relative to the ecliptic\n",
    "\n",
    "In both of these sentences only one word is changed i.e `angular standards` &#8594; `polar scales`.\n",
    "so to find similarity when these types of changes are done we can't use sentence vectorization and then checking similarity using euclidian/cosine similarity but rather we should move for **supervised learning** for accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa742f8-1f42-48cb-a585-b47afbc55957",
   "metadata": {},
   "source": [
    "# Using Parts Of Speech Tagging for Sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14b0e0-671c-402e-8e2f-f2eb922773b6",
   "metadata": {},
   "source": [
    "## Accuracy of the model is:55.800000000000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a93f111-7063-41e3-be27-1383c8b48bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>This was a series of nested angular standards ...</td>\n",
       "      <td>This was a series of nested polar scales , so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>His father emigrated to Missouri in 1868 but r...</td>\n",
       "      <td>His father emigrated to America in 1868 , but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>In January 2011 , the Deputy Secretary General...</td>\n",
       "      <td>In January 2011 , FIBA Asia deputy secretary g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Steiner argued that , in the right circumstanc...</td>\n",
       "      <td>Steiner held that the spiritual world can be r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Luciano Williames Dias ( born July 25 , 1970 )...</td>\n",
       "      <td>Luciano Williames Dias ( born 25 July 1970 ) i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentence1  \\\n",
       "0   1  This was a series of nested angular standards ...   \n",
       "1   2  His father emigrated to Missouri in 1868 but r...   \n",
       "2   3  In January 2011 , the Deputy Secretary General...   \n",
       "3   4  Steiner argued that , in the right circumstanc...   \n",
       "4   5  Luciano Williames Dias ( born July 25 , 1970 )...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  This was a series of nested polar scales , so ...      0  \n",
       "1  His father emigrated to America in 1868 , but ...      0  \n",
       "2  In January 2011 , FIBA Asia deputy secretary g...      1  \n",
       "3  Steiner held that the spiritual world can be r...      0  \n",
       "4  Luciano Williames Dias ( born 25 July 1970 ) i...      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from datasets import load_dataset\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "df = load_dataset('paws', 'labeled_final')\n",
    "\n",
    "train = df['train'].to_pandas()\n",
    "validation = df['validation'].to_pandas()\n",
    "test = df['test'].to_pandas()\n",
    "# train.head()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c881027-1966-4f05-9a94-e306014551aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4418736462824639"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = sum(train['label'] == 1)\n",
    "total_pairs = (max(train['id']))\n",
    "threshold = count/total_pairs\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4d6e9b-9ead-454a-884f-735e1f4d7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd050a9-f47d-4e6d-ab4d-ce9ca4e2600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/mzwwwjm533sgdkcfymq6ntdh0000gn/T/ipykernel_97050/1238688201.py:16: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score_adj = nlp(s1_adj).similarity(nlp(s2_adj))\n",
      "/var/folders/02/mzwwwjm533sgdkcfymq6ntdh0000gn/T/ipykernel_97050/1238688201.py:15: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score_verb = nlp(s1_verbs).similarity(nlp(s2_verbs))\n",
      "/var/folders/02/mzwwwjm533sgdkcfymq6ntdh0000gn/T/ipykernel_97050/1238688201.py:17: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score_noun = nlp(s1_nouns).similarity(nlp(s2_nouns))\n"
     ]
    }
   ],
   "source": [
    "correct_score = []\n",
    "# for i in range(2):\n",
    "for i in range((max(test['id']))):\n",
    "    s1 = nlp(test['sentence1'][i])\n",
    "    s1_verbs = \" \".join([token.lemma_ for token in s1 if token.pos_ == \"VERB\"])\n",
    "    s1_adj = \" \".join([token.lemma_ for token in s1 if token.pos_ == \"ADJ\"])\n",
    "    s1_nouns = \" \".join([token.lemma_ for token in s1 if token.pos_ == \"NOUN\"])\n",
    "    \n",
    "    s2 = nlp(test['sentence2'][i])\n",
    "    s2_verbs = \" \".join([token.lemma_ for token in s2 if token.pos_ == \"VERB\"])\n",
    "    s2_adj = \" \".join([token.lemma_ for token in s2 if token.pos_ == \"ADJ\"])\n",
    "    s2_nouns = \" \".join([token.lemma_ for token in s2 if token.pos_ == \"NOUN\"])\n",
    "    \n",
    "    similarity_score = s1.similarity(s2)\n",
    "    score_verb = nlp(s1_verbs).similarity(nlp(s2_verbs))\n",
    "    score_adj = nlp(s1_adj).similarity(nlp(s2_adj))\n",
    "    score_noun = nlp(s1_nouns).similarity(nlp(s2_nouns))\n",
    "    # print(f\"sentence1: {s1}\\n\")\n",
    "    # print(f\"nouns:{s1_nouns}\\t adj:{s1_adj}\\t verb:{s1_verbs}\\n\")\n",
    "    # print(f\"sentence2: {s2}\\n\")\n",
    "    # print(f\"nouns:{s2_nouns}\\t adj:{s2_adj}\\t verb:{s2_verbs}\\n\")\n",
    "    # print(f\"sim-score:{similarity_score}, adj_score:{score_adj}, noun-sc:{score_noun}, verb_score:{score_verb}\\n\")\n",
    "    # print(\"-------------------------\")\n",
    "    if similarity_score == threshold and score_noun == 1.0 and score_verb == 1.0 and score_adj == 1.0:\n",
    "        correct_score.append(1)\n",
    "    else:\n",
    "        correct_score.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c92785a-c22e-42d8-92b4-38ca17f30938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d3e1380-14af-4f91-b9b2-6aa4e3d2d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n",
      "Accuracy of the model is:55.800000000000004\n"
     ]
    }
   ],
   "source": [
    "actual_scores = np.array(test['label'])\n",
    "correct_score_arr = np.array(correct_score)\n",
    "predicted_scores = np.sum(correct_score_arr == actual_scores)\n",
    "print(predicted_scores)\n",
    "accuracy = predicted_scores / len(actual_scores)\n",
    "print(f\"Accuracy of the model is:{accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae4533-b384-4e86-9421-d0ea6eefdd49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_",
   "language": "python",
   "name": "nlp_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
